{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Complete SFT \u2192 DPO Pipeline with Model Comparison\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation & Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth  # Do this in local & cloud setups\n",
        "else:\n",
        "    import torch; v = re.match(r'[\\d]{1,}\\.[\\d]{1,}', str(torch.__version__)).group(0)\n",
        "    xformers = 'xformers==' + {'2.10':'0.0.34','2.9':'0.0.33.post1','2.8':'0.0.32.post2'}.get(v, \"0.0.34\")\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth_zoo bitsandbytes accelerate {xformers} peft trl triton unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2\n",
        "!pip install rouge-score bert-score"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "print(\"\u2705 All packages installed successfully!\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 All packages installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Base Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 4096\n",
        "dtype = None  # None for auto detection\n",
        "load_in_4bit = True\n",
        "\n",
        "# Load base model\n",
        "base_model, base_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Llama-3-8B\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "# Set chat template\n",
        "if base_tokenizer.chat_template is None:\n",
        "    base_tokenizer.chat_template = \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\\\n\\\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}{% endif %}\"\n",
        "\n",
        "print(\"\u2705 Base model loaded!\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83e\udda5 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "\ud83e\udda5 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2026.1.4: Fast Llama patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.494 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.34. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17a49c51b99e4a5fa9b4280df49c66d2",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f419242b36c145cb88ce7ec378bccaaa",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "generation_config.json:   0%|          | 0.00/198 [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6bef866419644298bb662051da7045c",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "tokenizer_config.json: 0.00B [00:00, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65a567b4850a4b44b68b7ffa38b3bd5e",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "tokenizer.json: 0.00B [00:00, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80a025f80ac249dfba7647a56128e846",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Base model loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Preparation & Splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"data 4.csv\")\n",
        "\n",
        "# Clean dataset - remove rows with null or empty values\n",
        "def is_blank(x):\n",
        "    return not isinstance(x, str) or x.strip() == \"\"\n",
        "\n",
        "df_clean = df[\n",
        "    df[\"text\"].apply(lambda x: isinstance(x, str) and x.strip() != \"\") &\n",
        "    df[\"summary\"].apply(lambda x: isinstance(x, str) and x.strip() != \"\") &\n",
        "    df[\"generated_summary\"].apply(lambda x: isinstance(x, str) and x.strip() != \"\")\n",
        "].reset_index(drop=True)\n",
        "\n",
        "print(f\"Original dataset size: {len(df)}\")\n",
        "print(f\"Cleaned dataset size: {len(df_clean)}\")\n",
        "print(f\"Dropped rows: {len(df) - len(df_clean)}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset size: 5000\n",
            "Cleaned dataset size: 5000\n",
            "Dropped rows: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Convert to Hugging Face dataset\n",
        "dataset = Dataset.from_pandas(df_clean)\n",
        "\n",
        "# Split: 60% SFT, 30% DPO, 10% Test\n",
        "# First split: 90% train, 10% test\n",
        "split1 = dataset.train_test_split(test_size=0.02, seed=42)\n",
        "test_dataset = split1['test']\n",
        "\n",
        "# Second split: From remaining 90%, split into 60% SFT and 30% DPO\n",
        "# This means 66.67% of the 90% goes to SFT, 33.33% goes to DPO\n",
        "split2 = split1['train'].train_test_split(test_size=0.3333, seed=42)\n",
        "sft_dataset = split2['train']\n",
        "dpo_dataset = split2['test']\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DATASET SPLIT SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total samples: {len(dataset)}\")\n",
        "print(f\"SFT training: {len(sft_dataset)} ({len(sft_dataset)/len(dataset)*100:.1f}%)\")\n",
        "print(f\"DPO training: {len(dpo_dataset)} ({len(dpo_dataset)/len(dataset)*100:.1f}%)\")\n",
        "print(f\"Test: {len(test_dataset)} ({len(test_dataset)/len(dataset)*100:.1f}%)\")\n",
        "print(\"=\"*60)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "DATASET SPLIT SUMMARY\n",
            "============================================================\n",
            "Total samples: 5000\n",
            "SFT training: 3266 (65.3%)\n",
            "DPO training: 1634 (32.7%)\n",
            "Test: 100 (2.0%)\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prepare SFT Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "def prepare_sft_format(examples):\n",
        "    \"\"\"Format data for supervised fine-tuning\"\"\"\n",
        "    messages_list = []\n",
        "    \n",
        "    for text, summary in zip(examples[\"text\"], examples[\"summary\"]):\n",
        "        user_prompt = (\n",
        "            \"You are an engaging writer.\\n\\n\"\n",
        "            \"A spotlight is a short narrative teaser written as a single paragraph. \"\n",
        "            \"It highlights ONE intriguing angle and sparks curiosity without summarizing.\\n\\n\"\n",
        "            \"Write a spotlight (1-2 sentences).\\n\\n\"\n",
        "            f\"### Document:\\n{text}\"\n",
        "        )\n",
        "        \n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "            {\"role\": \"assistant\", \"content\": summary.strip()},\n",
        "        ]\n",
        "        \n",
        "        messages_list.append(messages)\n",
        "    \n",
        "    return {\"messages\": messages_list}\n",
        "\n",
        "# Apply formatting\n",
        "sft_formatted = sft_dataset.map(\n",
        "    prepare_sft_format,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\", \"summary\", \"generated_summary\"],\n",
        "    desc=\"Formatting SFT data\",\n",
        ")\n",
        "\n",
        "print(f\"\\n\u2705 SFT dataset prepared: {len(sft_formatted)} samples\")\n",
        "print(\"\\nSample:\")\n",
        "print(sft_formatted[0]['messages'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07e41ea5034b4479a20d73bd57b7c4f0",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Formatting SFT data:   0%|          | 0/3266 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2705 SFT dataset prepared: 3266 samples\n",
            "\n",
            "Sample:\n",
            "[{'content': 'You are an engaging writer.\\n\\nA spotlight is a short narrative teaser written as a single paragraph. It highlights ONE intriguing angle and sparks curiosity without summarizing.\\n\\nWrite a spotlight (1-2 sentences).\\n\\n### Document:\\nLos Angeles (CNN) -- In the final two hours of a dramatic standoff with rogue ex-Los Angeles police officer Christopher Dorner, San Bernardino County sheriff\\'s deputies did not fire a single gunshot during their raid of a compound where he barricaded himself after killing one deputy and seriously wounding another, according to dispatch logs. When the SWAT team arrived on February 12, a robot-controlled tractor tore down blood-spattered walls of the vacated home near Big Bear, offering tactical teams a clean view inside the cabin, logs show. The redacted transcripts detail the chase that began after a 911 call from a Big Bear couple whom Dorner had held hostage at gunpoint and hogtied before fleeing their condo that Tuesday afternoon. The manhunt ended in Dorner\\'s death from a suspected self-inflicted gunshot wound. The printed text of dispatch logs began at 12:23 p.m. PT with a frantic cell-phone call to authorities from Karen Reynolds and her husband, Jim, who reported being \"tied up by Chris Dorner,\" who had fled in their vehicle 15 to 30 minutes earlier. According to the logs, about one hour later, the first exchange of gunfire with Dorner occurred when he was spotted by California Fish and Wildlife officers as he attempted to flee the mountain in another stolen vehicle. Women wounded in manhunt to get $4.2 million . When officers identified Dorner\\'s abandoned vehicle and traced him to a vacation cabin, a barrage of gunfire erupted, and hundreds of rounds were exchanged, authorities said. As the gun battle intensified, officers requested an armored vehicle and air support to protect and rescue two officers wounded in the shootout, logs showed. The wounded officers, sheriff\\'s Detective Jeremiah MacKay and Deputy Alex Collins, were loaded into the flatbed of a pickup truck and later airlifted to area hospitals, where MacKay was pronounced dead. Collins was seriously wounded. At 4:05 p.m. PT, police reported green smoke inside the cabin, allegedly set off by Dorner. Five minutes later, a sheriff\\'s tactical unit fired gas canisters into the cabin after Dorner refused to respond to commands to surrender, logs showed. At 4:20 p.m. PT, police dispatch logs reported the sound of a single gunshot from inside the residence. Authorities later said that shot sounded different from the many others than had come from inside the house. Couple recounts being under protection during Dorner killing spree . Moments later, the entire cabin was engulfed in flames, setting off hundreds of rounds of live ammunition purportedly left behind by Dorner. It would take several hours for authorities to safely enter the compound, where they found Dorner\\'s body in the basement. A preliminary examination concluded that his death was the result of a suspected self-inflicted gunshot wound. In the days after the incident, authorities defended their tactical strategy of firing gas \"burners\" into the cabin, stating that SWAT team officers had issued several unanswered commands for Dorner to surrender. The dispatch logs were released in response to a request from media organizations, including CNN. During the unprecedented manhunt that went as far as Tijuana, Mexico, the search for Dorner turned to Big Bear after his burning truck was found deserted on a local forest road. In his manifesto posted on Facebook, Dorner allegedly threatened \"unconventional and asymmetrical warfare\" against police. The manifesto was discovered three days after the slaying of an Irvine couple, Monica Quan and her fianc\u00c3\u00a9, Keith Lawrence. Quan was the daughter of a retired Los Angeles police captain whom Dorner allegedly blamed in part for his firing in 2009. While attempting to elude authorities, Dorner killed Riverside police Officer Michael Crain during an ambush and wounded three others. At the height of the search for Dorner, more than 200 officers scoured Big Bear Mountain, cabin by cabin.', 'role': 'user'}, {'content': \"San Bernardino County deputies didn't fire guns in last 2 hours of Dorner manhunt . Rogue ex-Los Angeles police officer refused to surrender, police logs show . Green smoke, allegedly set off by Dorner, was seen inside the Big Bear cabin, logs show .\", 'role': 'assistant'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Apply chat template for SFT\n",
        "def apply_sft_chat_template(example, tokenizer):\n",
        "    \"\"\"Apply chat template to messages\"\"\"\n",
        "    messages = example[\"messages\"]\n",
        "    \n",
        "    # Add empty system message if needed\n",
        "    if messages[0][\"role\"] != \"system\":\n",
        "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
        "    \n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False,\n",
        "    )\n",
        "    \n",
        "    return {\"text\": text}\n",
        "\n",
        "sft_formatted = sft_formatted.map(\n",
        "    lambda x: apply_sft_chat_template(x, base_tokenizer),\n",
        "    remove_columns=[\"messages\"],\n",
        "    desc=\"Applying chat template\",\n",
        ")\n",
        "\n",
        "print(\"\\n\u2705 Chat template applied\")\n",
        "print(\"\\nSample formatted text:\")\n",
        "print(sft_formatted[0]['text'][:500] + \"...\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bc345c05f034c70862aee6398bcf248",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Applying chat template:   0%|          | 0/3266 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2705 Chat template applied\n",
            "\n",
            "Sample formatted text:\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "You are an engaging writer.\n",
            "\n",
            "A spotlight is a short narrative teaser written as a single paragraph. It highlights ONE intriguing angle and sparks curiosity without summarizing.\n",
            "\n",
            "Write a spotlight (1-2 sentences).\n",
            "\n",
            "### Document:\n",
            "Los Angeles (CNN) -- In the final two hours of a dramatic standoff with rogue ex-Los Angeles police officer Christopher Dorner, San Bernardino County sheriff's...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Stage 1: Supervised Fine-Tuning (SFT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Prepare model for SFT\n",
        "sft_model, sft_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Llama-3-8B\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "# Set chat template\n",
        "if sft_tokenizer.chat_template is None:\n",
        "    sft_tokenizer.chat_template = base_tokenizer.chat_template\n",
        "\n",
        "# Add LoRA adapters\n",
        "sft_model = FastLanguageModel.get_peft_model(\n",
        "    sft_model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "print(\"\\n\u2705 SFT model prepared with LoRA adapters\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2026.1.4: Fast Llama patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.494 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.34. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2026.1.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2705 SFT model prepared with LoRA adapters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "sft_trainer = SFTTrainer(\n",
        "    model=sft_model,\n",
        "    tokenizer=sft_tokenizer,\n",
        "    train_dataset=sft_formatted,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    packing=False,\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_ratio=0.1,\n",
        "        num_train_epochs=2,\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=3407,\n",
        "        output_dir=\"sft_outputs\",\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING SFT TRAINING\")\n",
        "print(\"=\"*60)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2272a99285c54ad28f69af503075ab2e",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Unsloth: Tokenizing [\"text\"] (num_proc=64):   0%|          | 0/3266 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[accelerate.utils.other|WARNING]Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83e\udda5 Unsloth: Padding-free auto-enabled, enabling faster training.\n",
            "\n",
            "============================================================\n",
            "STARTING SFT TRAINING\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Train SFT model\n",
        "sft_stats = sft_trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SFT TRAINING COMPLETED\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Training time: {sft_stats.metrics['train_runtime']:.2f} seconds\")\n",
        "print(f\"Training time: {sft_stats.metrics['train_runtime']/60:.2f} minutes\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 3,266 | Num Epochs = 2 | Total steps = 410\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='410' max='410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [410/410 29:54, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.407000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.189200</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.984300</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.920400</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.929800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.896400</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.913700</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.898600</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.897000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.876600</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.895900</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.908000</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.910700</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.926300</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.928600</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.883000</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.890100</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.911700</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.910600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.914200</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.868100</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.818500</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.812300</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.794400</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.804800</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.778200</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>1.806800</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.799800</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>1.797600</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.751600</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.794900</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.808900</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>1.815500</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.826100</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.819900</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.798500</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>1.818900</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>1.797400</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>1.827800</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.784900</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>1.793700</td>\n    </tr>\n  </tbody>\n</table><p>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SFT TRAINING COMPLETED\n",
            "============================================================\n",
            "Training time: 1807.74 seconds\n",
            "Training time: 30.13 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Save SFT model\n",
        "sft_model.save_pretrained(\"llama3_spotlight_sft\")\n",
        "sft_tokenizer.save_pretrained(\"llama3_spotlight_sft\")\n",
        "\n",
        "print(\"\\n\u2705 SFT model saved to 'llama3_spotlight_sft'\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2705 SFT model saved to 'llama3_spotlight_sft'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Prepare DPO Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import re\n",
        "from typing import Literal\n",
        "\n",
        "def apply_chat_template(\n",
        "    example,\n",
        "    tokenizer,\n",
        "    task: Literal[\"sft\", \"generation\", \"rm\", \"dpo\"] = \"sft\",\n",
        "    assistant_prefix=\"<|assistant|>\\n\",\n",
        "):\n",
        "    def _strip_prefix(s, pattern):\n",
        "        return re.sub(f\"^{re.escape(pattern)}\", \"\", s)\n",
        "\n",
        "    if task == \"dpo\":\n",
        "        if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
        "            # Extract prompt messages\n",
        "            prompt_messages = [\n",
        "                [msg for msg in example[\"chosen\"] if msg[\"role\"] == \"user\"][0]\n",
        "            ]\n",
        "            \n",
        "            # Insert system message if needed\n",
        "            if example[\"chosen\"][0][\"role\"] != \"system\":\n",
        "                prompt_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
        "            else:\n",
        "                prompt_messages.insert(0, example[\"chosen\"][0])\n",
        "            \n",
        "            chosen_messages = example[\"chosen\"][1:]\n",
        "            rejected_messages = example[\"rejected\"][1:]\n",
        "            \n",
        "            example[\"text_chosen\"] = tokenizer.apply_chat_template(\n",
        "                chosen_messages, tokenize=False\n",
        "            )\n",
        "            example[\"text_rejected\"] = tokenizer.apply_chat_template(\n",
        "                rejected_messages, tokenize=False\n",
        "            )\n",
        "            example[\"text_prompt\"] = tokenizer.apply_chat_template(\n",
        "                prompt_messages, tokenize=False, add_generation_prompt=True\n",
        "            )\n",
        "            \n",
        "            example[\"text_chosen\"] = _strip_prefix(\n",
        "                example[\"text_chosen\"], assistant_prefix\n",
        "            )\n",
        "            example[\"text_rejected\"] = _strip_prefix(\n",
        "                example[\"text_rejected\"], assistant_prefix\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Could not format example as dialogue for `dpo` task! Require `[chosen, rejected]` keys but found {list(example.keys())}\"\n",
        "            )\n",
        "    \n",
        "    return example"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "def prepare_dpo_format(examples):\n",
        "    \"\"\"Prepare data for DPO training\"\"\"\n",
        "    chosen_messages = []\n",
        "    rejected_messages = []\n",
        "\n",
        "    for text, summary, generated_summary in zip(\n",
        "        examples[\"text\"],\n",
        "        examples[\"summary\"],\n",
        "        examples[\"generated_summary\"]\n",
        "    ):\n",
        "        generated_summary_cleaned = generated_summary.replace(\"[SUMMARY]\", \"\").strip()\n",
        "\n",
        "        user_prompt = (\n",
        "            \"You are an engaging writer.\\n\\n\"\n",
        "            \"A spotlight is a short narrative teaser written as a single paragraph. \"\n",
        "            \"It highlights ONE intriguing angle and sparks curiosity without summarizing.\\n\\n\"\n",
        "            \"Write a spotlight (1-2 sentences).\\n\\n\"\n",
        "            f\"### Document:\\n{text}\"\n",
        "        )\n",
        "\n",
        "        # Chosen: human-written summary (better)\n",
        "        chosen_messages.append([\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "            {\"role\": \"assistant\", \"content\": summary.strip()},\n",
        "        ])\n",
        "\n",
        "        # Rejected: generated summary (worse)\n",
        "        rejected_messages.append([\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "            {\"role\": \"assistant\", \"content\": generated_summary_cleaned},\n",
        "        ])\n",
        "\n",
        "    return {\n",
        "        \"chosen\": chosen_messages,\n",
        "        \"rejected\": rejected_messages,\n",
        "    }\n",
        "\n",
        "# Apply DPO formatting\n",
        "dpo_formatted = dpo_dataset.map(\n",
        "    prepare_dpo_format,\n",
        "    batched=True,\n",
        "    num_proc=4,\n",
        "    remove_columns=[\"text\", \"summary\", \"generated_summary\"],\n",
        "    desc=\"Preparing DPO format\",\n",
        ")\n",
        "\n",
        "print(f\"\\n\u2705 DPO dataset prepared: {len(dpo_formatted)} samples\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ecece5ea9a904aadb9d648c10e409996",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Preparing DPO format (num_proc=4):   0%|          | 0/1634 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2705 DPO dataset prepared: 1634 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Apply chat template for DPO\n",
        "column_names = list(dpo_formatted.features)\n",
        "dpo_formatted = dpo_formatted.map(\n",
        "    apply_chat_template,\n",
        "    fn_kwargs={\"tokenizer\": sft_tokenizer, \"task\": \"dpo\"},\n",
        "    num_proc=4,\n",
        "    remove_columns=column_names,\n",
        "    desc=\"Applying chat template for DPO\",\n",
        ")\n",
        "\n",
        "# Rename columns\n",
        "dpo_formatted = dpo_formatted.rename_columns(\n",
        "    {\n",
        "        \"text_prompt\": \"prompt\",\n",
        "        \"text_chosen\": \"chosen\",\n",
        "        \"text_rejected\": \"rejected\",\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"\\n\u2705 DPO dataset formatted\")\n",
        "print(\"\\nSample DPO data:\")\n",
        "print(f\"Prompt length: {len(dpo_formatted[0]['prompt'])}\")\n",
        "print(f\"Chosen length: {len(dpo_formatted[0]['chosen'])}\")\n",
        "print(f\"Rejected length: {len(dpo_formatted[0]['rejected'])}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59beaf4dd7ff46bfa44af22db8afd8c5",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Applying chat template for DPO (num_proc=4):   0%|          | 0/1634 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2705 DPO dataset formatted\n",
            "\n",
            "Sample DPO data:\n",
            "Prompt length: 3122\n",
            "Chosen length: 278\n",
            "Rejected length: 345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Stage 2: Direct Preference Optimization (DPO)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Load the SFT model for DPO training\n",
        "from unsloth import PatchDPOTrainer\n",
        "PatchDPOTrainer()\n",
        "\n",
        "dpo_model, dpo_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"llama3_spotlight_sft\",  # Load our saved SFT model\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "# Set chat template\n",
        "if dpo_tokenizer.chat_template is None:\n",
        "    dpo_tokenizer.chat_template = sft_tokenizer.chat_template\n",
        "\n",
        "print(\"\\n\u2705 SFT model loaded for DPO training\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2026.1.4: Fast Llama patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.494 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.34. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "\n",
            "\u2705 SFT model loaded for DPO training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Add LoRA for DPO (on top of SFT)\n",
        "dpo_model = FastLanguageModel.get_peft_model(\n",
        "    dpo_model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "print(\"\u2705 LoRA adapters added for DPO\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 LoRA adapters added for DPO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "from trl import DPOTrainer, DPOConfig\n",
        "\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model=dpo_model,\n",
        "    ref_model=None,  # Use implicit reference model\n",
        "    args=DPOConfig(\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_ratio=0.1,\n",
        "        num_train_epochs=2,\n",
        "        learning_rate=5e-6,\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.0,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=42,\n",
        "        output_dir=\"dpo_outputs\",\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        "    beta=0.1,\n",
        "    train_dataset=dpo_formatted,\n",
        "    tokenizer=dpo_tokenizer,\n",
        "    max_length=1024,\n",
        "    max_prompt_length=512,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING DPO TRAINING\")\n",
        "print(\"=\"*60)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "786e8feebbd3481fb1e9157afd34a2da",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Extracting prompt in train dataset (num_proc=64):   0%|          | 0/1634 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f994f4a519984af7bf53417986417397",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Applying chat template to train dataset (num_proc=64):   0%|          | 0/1634 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4aa3bab9f12348d8a22a368e2b85d106",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Tokenizing train dataset (num_proc=64):   0%|          | 0/1634 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[accelerate.utils.other|WARNING]Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "STARTING DPO TRAINING\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Train DPO model\n",
        "dpo_stats = dpo_trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DPO TRAINING COMPLETED\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Training time: {dpo_stats.metrics['train_runtime']:.2f} seconds\")\n",
        "print(f\"Training time: {dpo_stats.metrics['train_runtime']/60:.2f} minutes\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 1,634 | Num Epochs = 2 | Total steps = 206\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='206' max='206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [206/206 24:10, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>rewards / chosen</th>\n      <th>rewards / rejected</th>\n      <th>rewards / accuracies</th>\n      <th>rewards / margins</th>\n      <th>logps / chosen</th>\n      <th>logps / rejected</th>\n      <th>logits / chosen</th>\n      <th>logits / rejected</th>\n      <th>eval_logits / chosen</th>\n      <th>eval_logits / rejected</th>\n      <th>nll_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.209600</td>\n      <td>1.830216</td>\n      <td>-0.013214</td>\n      <td>0.962500</td>\n      <td>1.843430</td>\n      <td>-196.439774</td>\n      <td>-182.500626</td>\n      <td>-0.564625</td>\n      <td>-0.539653</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.158400</td>\n      <td>1.677019</td>\n      <td>-0.505755</td>\n      <td>0.987500</td>\n      <td>2.182774</td>\n      <td>-200.013397</td>\n      <td>-192.230576</td>\n      <td>-0.551604</td>\n      <td>-0.513899</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.079800</td>\n      <td>1.997966</td>\n      <td>-1.362199</td>\n      <td>0.987500</td>\n      <td>3.360165</td>\n      <td>-199.118530</td>\n      <td>-216.445068</td>\n      <td>-0.529569</td>\n      <td>-0.513943</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.015800</td>\n      <td>2.127985</td>\n      <td>-2.785750</td>\n      <td>1.000000</td>\n      <td>4.913735</td>\n      <td>-199.669098</td>\n      <td>-225.341553</td>\n      <td>-0.557298</td>\n      <td>-0.558136</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.007800</td>\n      <td>1.998144</td>\n      <td>-4.298892</td>\n      <td>1.000000</td>\n      <td>6.297035</td>\n      <td>-199.584793</td>\n      <td>-231.169479</td>\n      <td>-0.533040</td>\n      <td>-0.501671</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.009900</td>\n      <td>1.623006</td>\n      <td>-5.300343</td>\n      <td>1.000000</td>\n      <td>6.923349</td>\n      <td>-205.200729</td>\n      <td>-250.791183</td>\n      <td>-0.552743</td>\n      <td>-0.545036</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.004200</td>\n      <td>1.780026</td>\n      <td>-5.800466</td>\n      <td>1.000000</td>\n      <td>7.580492</td>\n      <td>-202.441803</td>\n      <td>-248.020660</td>\n      <td>-0.599831</td>\n      <td>-0.532425</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.003000</td>\n      <td>1.534922</td>\n      <td>-6.475748</td>\n      <td>1.000000</td>\n      <td>8.010670</td>\n      <td>-202.425674</td>\n      <td>-249.315720</td>\n      <td>-0.559006</td>\n      <td>-0.517942</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.048200</td>\n      <td>1.288625</td>\n      <td>-6.994647</td>\n      <td>0.993750</td>\n      <td>8.283270</td>\n      <td>-211.590485</td>\n      <td>-290.934631</td>\n      <td>-0.595162</td>\n      <td>-0.576117</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.004800</td>\n      <td>1.560372</td>\n      <td>-7.058969</td>\n      <td>1.000000</td>\n      <td>8.619341</td>\n      <td>-208.779877</td>\n      <td>-269.996307</td>\n      <td>-0.578668</td>\n      <td>-0.545053</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.005600</td>\n      <td>1.629597</td>\n      <td>-7.106162</td>\n      <td>0.993243</td>\n      <td>8.735758</td>\n      <td>-205.897385</td>\n      <td>-255.037292</td>\n      <td>-0.600124</td>\n      <td>-0.547533</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.004100</td>\n      <td>1.553706</td>\n      <td>-7.196902</td>\n      <td>1.000000</td>\n      <td>8.750608</td>\n      <td>-203.617233</td>\n      <td>-273.808197</td>\n      <td>-0.556215</td>\n      <td>-0.583223</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.001700</td>\n      <td>1.734018</td>\n      <td>-7.419870</td>\n      <td>1.000000</td>\n      <td>9.153888</td>\n      <td>-200.310211</td>\n      <td>-258.917694</td>\n      <td>-0.522710</td>\n      <td>-0.470286</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.002600</td>\n      <td>1.830781</td>\n      <td>-7.419763</td>\n      <td>1.000000</td>\n      <td>9.250544</td>\n      <td>-206.408081</td>\n      <td>-303.583130</td>\n      <td>-0.588332</td>\n      <td>-0.580656</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.000800</td>\n      <td>2.084733</td>\n      <td>-7.384931</td>\n      <td>1.000000</td>\n      <td>9.469664</td>\n      <td>-197.281372</td>\n      <td>-261.914062</td>\n      <td>-0.546369</td>\n      <td>-0.508337</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.001300</td>\n      <td>2.033281</td>\n      <td>-7.029619</td>\n      <td>1.000000</td>\n      <td>9.062900</td>\n      <td>-196.940720</td>\n      <td>-257.349396</td>\n      <td>-0.553907</td>\n      <td>-0.528319</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.000800</td>\n      <td>1.912218</td>\n      <td>-7.465610</td>\n      <td>1.000000</td>\n      <td>9.377828</td>\n      <td>-203.173569</td>\n      <td>-281.275391</td>\n      <td>-0.581364</td>\n      <td>-0.550912</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.002600</td>\n      <td>1.933861</td>\n      <td>-7.252433</td>\n      <td>1.000000</td>\n      <td>9.186295</td>\n      <td>-199.378662</td>\n      <td>-266.638611</td>\n      <td>-0.570350</td>\n      <td>-0.514240</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.000700</td>\n      <td>1.877601</td>\n      <td>-7.229300</td>\n      <td>1.000000</td>\n      <td>9.106901</td>\n      <td>-201.903595</td>\n      <td>-258.993805</td>\n      <td>-0.508210</td>\n      <td>-0.478175</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.029600</td>\n      <td>1.976835</td>\n      <td>-6.973709</td>\n      <td>0.993750</td>\n      <td>8.950544</td>\n      <td>-203.745026</td>\n      <td>-252.256744</td>\n      <td>-0.560691</td>\n      <td>-0.484049</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n  </tbody>\n</table><p>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "DPO TRAINING COMPLETED\n",
            "============================================================\n",
            "Training time: 1461.68 seconds\n",
            "Training time: 24.36 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Save DPO model\n",
        "dpo_model.save_pretrained(\"llama3_spotlight_sft_dpo\")\n",
        "dpo_tokenizer.save_pretrained(\"llama3_spotlight_sft_dpo\")\n",
        "\n",
        "print(\"\\n\u2705 SFT+DPO model saved to 'llama3_spotlight_sft_dpo'\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2705 SFT+DPO model saved to 'llama3_spotlight_sft_dpo'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Prepare Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Prepare test dataset in DPO format (to get prompts and references)\n",
        "test_formatted = test_dataset.map(\n",
        "    prepare_dpo_format,\n",
        "    batched=True,\n",
        "    num_proc=4,\n",
        "    remove_columns=[\"text\", \"summary\", \"generated_summary\"],\n",
        "    desc=\"Preparing test data\",\n",
        ")\n",
        "\n",
        "column_names = list(test_formatted.features)\n",
        "test_formatted = test_formatted.map(\n",
        "    apply_chat_template,\n",
        "    fn_kwargs={\"tokenizer\": dpo_tokenizer, \"task\": \"dpo\"},\n",
        "    num_proc=4,\n",
        "    remove_columns=column_names,\n",
        "    desc=\"Applying chat template for test\",\n",
        ")\n",
        "\n",
        "test_formatted = test_formatted.rename_columns(\n",
        "    {\n",
        "        \"text_prompt\": \"prompt\",\n",
        "        \"text_chosen\": \"chosen\",\n",
        "        \"text_rejected\": \"rejected\",\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"\\n\u2705 Test dataset prepared: {len(test_formatted)} samples\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6e64264c6fe41dfa4b4edbc7977a2c3",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Preparing test data (num_proc=4):   0%|          | 0/100 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bb52b3bcb024d95b48e6a16544f5871",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Applying chat template for test (num_proc=4):   0%|          | 0/100 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2705 Test dataset prepared: 100 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Comparison: Base vs SFT vs SFT+DPO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LOADING ALL MODELS FOR COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Load Base Model\n",
        "print(\"\\n1. Loading Base Model...\")\n",
        "base_model_eval, base_tokenizer_eval = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Llama-3-8B\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "if base_tokenizer_eval.chat_template is None:\n",
        "    base_tokenizer_eval.chat_template = dpo_tokenizer.chat_template\n",
        "FastLanguageModel.for_inference(base_model_eval)\n",
        "print(\"\u2705 Base model loaded\")\n",
        "\n",
        "# 2. Load SFT Model\n",
        "print(\"\\n2. Loading SFT Model...\")\n",
        "sft_model_eval, sft_tokenizer_eval = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"llama3_spotlight_sft\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "FastLanguageModel.for_inference(sft_model_eval)\n",
        "print(\"\u2705 SFT model loaded\")\n",
        "\n",
        "# 3. Load SFT+DPO Model\n",
        "print(\"\\n3. Loading SFT+DPO Model...\")\n",
        "sft_dpo_model_eval, sft_dpo_tokenizer_eval = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"llama3_spotlight_sft_dpo\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "FastLanguageModel.for_inference(sft_dpo_model_eval)\n",
        "print(\"\u2705 SFT+DPO model loaded\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ALL MODELS LOADED AND READY FOR EVALUATION\")\n",
        "print(\"=\"*80)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "LOADING ALL MODELS FOR COMPARISON\n",
            "================================================================================\n",
            "\n",
            "1. Loading Base Model...\n",
            "==((====))==  Unsloth 2026.1.4: Fast Llama patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.494 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.34. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "\u2705 Base model loaded\n",
            "\n",
            "2. Loading SFT Model...\n",
            "==((====))==  Unsloth 2026.1.4: Fast Llama patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.494 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.34. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "\u2705 SFT model loaded\n",
            "\n",
            "3. Loading SFT+DPO Model...\n",
            "==((====))==  Unsloth 2026.1.4: Fast Llama patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.494 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.34. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "\u2705 SFT+DPO model loaded\n",
            "\n",
            "================================================================================\n",
            "ALL MODELS LOADED AND READY FOR EVALUATION\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def generate_spotlight(model, tokenizer, prompt_text, max_new_tokens=256):\n",
        "    \"\"\"Generate spotlight given the formatted prompt\"\"\"\n",
        "    inputs = tokenizer(\n",
        "        prompt_text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=1024,\n",
        "    ).to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    \n",
        "    generated_text = tokenizer.decode(\n",
        "        outputs[0][inputs['input_ids'].shape[1]:],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    \n",
        "    return generated_text.strip()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RUNNING INFERENCE ON TEST SET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "num_samples = len(test_formatted)\n",
        "print(f\"\\nEvaluating on {num_samples} test samples...\\n\")\n",
        "\n",
        "base_predictions = []\n",
        "sft_predictions = []\n",
        "sft_dpo_predictions = []\n",
        "references = []\n",
        "\n",
        "for i in tqdm(range(num_samples), desc=\"Generating predictions\"):\n",
        "    example = test_formatted[i]\n",
        "    prompt_text = example[\"prompt\"]\n",
        "    ref_text = example[\"chosen\"]\n",
        "    \n",
        "    # Generate with base model\n",
        "    base_pred = generate_spotlight(base_model_eval, base_tokenizer_eval, prompt_text)\n",
        "    \n",
        "    # Generate with SFT model\n",
        "    sft_pred = generate_spotlight(sft_model_eval, sft_tokenizer_eval, prompt_text)\n",
        "    \n",
        "    # Generate with SFT+DPO model\n",
        "    sft_dpo_pred = generate_spotlight(sft_dpo_model_eval, sft_dpo_tokenizer_eval, prompt_text)\n",
        "    \n",
        "    references.append(ref_text)\n",
        "    base_predictions.append(base_pred)\n",
        "    sft_predictions.append(sft_pred)\n",
        "    sft_dpo_predictions.append(sft_dpo_pred)\n",
        "\n",
        "print(f\"\\n\u2705 Generated {len(base_predictions)} predictions for each model!\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "RUNNING INFERENCE ON TEST SET\n",
            "================================================================================\n",
            "\n",
            "Evaluating on 100 test samples...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating predictions:   0%|                                                          | 0/100 [00:00<?, ?it/s]\rGenerating predictions:   1%|\u258c                                                 | 1/100 [00:26<43:34, 26.41s/it]\rGenerating predictions:   2%|\u2588                                                 | 2/100 [01:01<51:40, 31.64s/it]\rGenerating predictions:   3%|\u2588\u258c                                                | 3/100 [01:36<53:33, 33.13s/it]\rGenerating predictions:   4%|\u2588\u2588                                                | 4/100 [02:02<48:38, 30.40s/it]\rGenerating predictions:   5%|\u2588\u2588\u258c                                               | 5/100 [02:33<48:00, 30.32s/it]\rGenerating predictions:   6%|\u2588\u2588\u2588                                               | 6/100 [03:07<49:59, 31.91s/it]\rGenerating predictions:   7%|\u2588\u2588\u2588\u258c                                              | 7/100 [03:43<51:17, 33.09s/it]\rGenerating predictions:   8%|\u2588\u2588\u2588\u2588                                              | 8/100 [04:18<51:50, 33.82s/it]\rGenerating predictions:   9%|\u2588\u2588\u2588\u2588\u258c                                             | 9/100 [04:44<47:33, 31.36s/it]\rGenerating predictions:  10%|\u2588\u2588\u2588\u2588\u2589                                            | 10/100 [05:20<48:50, 32.57s/it]\rGenerating predictions:  11%|\u2588\u2588\u2588\u2588\u2588\u258d                                           | 11/100 [05:46<45:29, 30.67s/it]\rGenerating predictions:  12%|\u2588\u2588\u2588\u2588\u2588\u2589                                           | 12/100 [06:17<45:20, 30.91s/it]\rGenerating predictions:  13%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                          | 13/100 [06:53<46:57, 32.39s/it]\rGenerating predictions:  14%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                          | 14/100 [07:19<43:42, 30.50s/it]\rGenerating predictions:  15%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                         | 15/100 [07:51<43:37, 30.79s/it]\rGenerating predictions:  16%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                         | 16/100 [08:27<45:23, 32.43s/it]\rGenerating predictions:  17%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                        | 17/100 [09:02<45:59, 33.25s/it]\rGenerating predictions:  18%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                        | 18/100 [09:37<46:09, 33.78s/it]\rGenerating predictions:  19%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                       | 19/100 [10:12<46:01, 34.09s/it]\rGenerating predictions:  20%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                       | 20/100 [10:47<45:49, 34.37s/it]\rGenerating predictions:  21%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                      | 21/100 [11:23<45:42, 34.72s/it]\rGenerating predictions:  22%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                      | 22/100 [11:55<44:12, 34.01s/it]\rGenerating predictions:  23%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                     | 23/100 [12:21<40:30, 31.56s/it]\rGenerating predictions:  24%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                     | 24/100 [12:56<41:18, 32.61s/it]\rGenerating predictions:  25%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                    | 25/100 [13:31<41:42, 33.37s/it]\rGenerating predictions:  26%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                    | 26/100 [14:06<41:48, 33.89s/it]\rGenerating predictions:  27%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                   | 27/100 [14:41<41:36, 34.20s/it]\rGenerating predictions:  28%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                   | 28/100 [15:16<41:14, 34.37s/it]\rGenerating predictions:  29%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                  | 29/100 [15:51<40:53, 34.55s/it]\rGenerating predictions:  30%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                  | 30/100 [16:17<37:23, 32.06s/it]\rGenerating predictions:  31%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                 | 31/100 [16:43<34:44, 30.21s/it]\rGenerating predictions:  32%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                 | 32/100 [17:09<32:42, 28.86s/it]\rGenerating predictions:  33%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                | 33/100 [17:44<34:17, 30.71s/it]\rGenerating predictions:  34%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                | 34/100 [18:10<32:13, 29.30s/it]\rGenerating predictions:  35%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                               | 35/100 [18:39<31:51, 29.41s/it]\rGenerating predictions:  36%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                               | 36/100 [19:14<33:10, 31.10s/it]\rGenerating predictions:  37%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                              | 37/100 [19:41<31:06, 29.63s/it]\rGenerating predictions:  38%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                              | 38/100 [20:07<29:31, 28.58s/it]\rGenerating predictions:  39%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                              | 39/100 [20:42<31:02, 30.53s/it]\rGenerating predictions:  40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                             | 40/100 [21:17<32:01, 32.02s/it]\rGenerating predictions:  41%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                             | 41/100 [21:53<32:30, 33.06s/it]\rGenerating predictions:  42%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                            | 42/100 [22:29<32:44, 33.88s/it]\rGenerating predictions:  43%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                            | 43/100 [23:04<32:39, 34.37s/it]\rGenerating predictions:  44%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                           | 44/100 [23:32<30:12, 32.37s/it]\rGenerating predictions:  45%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                           | 45/100 [24:01<28:56, 31.58s/it]\rGenerating predictions:  46%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                          | 46/100 [24:29<27:21, 30.40s/it]\rGenerating predictions:  47%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                          | 47/100 [24:55<25:46, 29.17s/it]\rGenerating predictions:  48%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                         | 48/100 [25:31<26:51, 30.99s/it]\rGenerating predictions:  49%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                         | 49/100 [26:06<27:32, 32.41s/it]\rGenerating predictions:  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                        | 50/100 [26:35<26:09, 31.39s/it]\rGenerating predictions:  51%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                        | 51/100 [27:03<24:46, 30.33s/it]\rGenerating predictions:  52%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                       | 52/100 [27:29<23:14, 29.04s/it]\rGenerating predictions:  53%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                       | 53/100 [28:04<24:09, 30.84s/it]\rGenerating predictions:  54%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                      | 54/100 [28:38<24:20, 31.74s/it]\rGenerating predictions:  55%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                      | 55/100 [29:04<22:27, 29.93s/it]\rGenerating predictions:  56%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                     | 56/100 [29:30<21:10, 28.88s/it]\rGenerating predictions:  57%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                     | 57/100 [30:00<20:45, 28.97s/it]\rGenerating predictions:  58%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                    | 58/100 [30:25<19:38, 28.06s/it]\rGenerating predictions:  59%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                    | 59/100 [31:01<20:45, 30.37s/it]\rGenerating predictions:  60%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                   | 60/100 [31:36<21:13, 31.84s/it]\rGenerating predictions:  61%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                   | 61/100 [32:04<19:48, 30.47s/it]\rGenerating predictions:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                  | 62/100 [32:39<20:10, 31.87s/it]\rGenerating predictions:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                  | 63/100 [33:05<18:30, 30.02s/it]\rGenerating predictions:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                 | 64/100 [33:32<17:29, 29.15s/it]\rGenerating predictions:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                 | 65/100 [34:07<18:04, 30.98s/it]\rGenerating predictions:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                | 66/100 [34:35<17:07, 30.22s/it]\rGenerating predictions:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                | 67/100 [35:06<16:40, 30.31s/it]\rGenerating predictions:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e               | 68/100 [35:34<15:46, 29.58s/it]\rGenerating predictions:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a               | 69/100 [36:09<16:09, 31.29s/it]\rGenerating predictions:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e              | 70/100 [36:44<16:14, 32.48s/it]\rGenerating predictions:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a              | 71/100 [37:19<16:02, 33.20s/it]\rGenerating predictions:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e             | 72/100 [37:45<14:26, 30.94s/it]\rGenerating predictions:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a             | 73/100 [38:20<14:28, 32.15s/it]\rGenerating predictions:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e            | 74/100 [38:55<14:17, 32.97s/it]\rGenerating predictions:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a            | 75/100 [39:20<12:48, 30.73s/it]\rGenerating predictions:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f           | 76/100 [39:46<11:40, 29.19s/it]\rGenerating predictions:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b           | 77/100 [40:21<11:49, 30.87s/it]\rGenerating predictions:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f          | 78/100 [40:46<10:45, 29.36s/it]\rGenerating predictions:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b          | 79/100 [41:12<09:54, 28.30s/it]\rGenerating predictions:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f         | 80/100 [41:47<10:07, 30.37s/it]\rGenerating predictions:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b         | 81/100 [42:23<10:05, 31.89s/it]\rGenerating predictions:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f        | 82/100 [42:49<09:02, 30.15s/it]\rGenerating predictions:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b        | 83/100 [43:15<08:10, 28.85s/it]\rGenerating predictions:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f       | 84/100 [43:41<07:27, 27.97s/it]\rGenerating predictions:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b       | 85/100 [44:11<07:11, 28.79s/it]\rGenerating predictions:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f      | 86/100 [44:46<07:09, 30.66s/it]\rGenerating predictions:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b      | 87/100 [45:17<06:38, 30.65s/it]\rGenerating predictions:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588      | 88/100 [45:52<06:23, 31.96s/it]\rGenerating predictions:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c     | 89/100 [46:27<06:02, 32.98s/it]\rGenerating predictions:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     | 90/100 [47:03<05:36, 33.67s/it]\rGenerating predictions:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c    | 91/100 [47:28<04:41, 31.27s/it]\rGenerating predictions:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    | 92/100 [48:04<04:19, 32.48s/it]\rGenerating predictions:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 93/100 [48:30<03:33, 30.48s/it]\rGenerating predictions:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 94/100 [49:03<03:07, 31.27s/it]\rGenerating predictions:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 95/100 [49:36<02:39, 31.84s/it]\rGenerating predictions:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 96/100 [50:02<02:00, 30.03s/it]\rGenerating predictions:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 97/100 [50:27<01:26, 28.75s/it]\rGenerating predictions:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 98/100 [50:53<00:55, 27.85s/it]\rGenerating predictions:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 99/100 [51:28<00:30, 30.04s/it]\rGenerating predictions: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [51:55<00:00, 29.15s/it]\rGenerating predictions: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [51:55<00:00, 31.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2705 Generated 100 predictions for each model!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Calculate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CALCULATING ROUGE SCORES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "def calculate_rouge_scores(predictions, references):\n",
        "    \"\"\"Calculate average ROUGE scores\"\"\"\n",
        "    rouge1_scores = []\n",
        "    rouge2_scores = []\n",
        "    rougeL_scores = []\n",
        "    \n",
        "    for pred, ref in zip(predictions, references):\n",
        "        scores = scorer.score(ref, pred)\n",
        "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
        "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "    \n",
        "    return {\n",
        "        'rouge1': sum(rouge1_scores) / len(rouge1_scores),\n",
        "        'rouge2': sum(rouge2_scores) / len(rouge2_scores),\n",
        "        'rougeL': sum(rougeL_scores) / len(rougeL_scores),\n",
        "    }\n",
        "\n",
        "base_rouge = calculate_rouge_scores(base_predictions, references)\n",
        "sft_rouge = calculate_rouge_scores(sft_predictions, references)\n",
        "sft_dpo_rouge = calculate_rouge_scores(sft_dpo_predictions, references)\n",
        "\n",
        "print(\"\\n\ud83d\udcca ROUGE SCORES COMPARISON:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'Metric':<15} {'Base':<15} {'SFT':<15} {'SFT+DPO':<15}\")\n",
        "print(\"-\" * 80)\n",
        "for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
        "    print(f\"{metric:<15} {base_rouge[metric]:<15.4f} {sft_rouge[metric]:<15.4f} {sft_dpo_rouge[metric]:<15.4f}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print(\"\\n\ud83d\udcc8 IMPROVEMENT OVER BASE:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'Metric':<15} {'SFT Improvement':<20} {'SFT+DPO Improvement':<25}\")\n",
        "print(\"-\" * 80)\n",
        "for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
        "    sft_imp = ((sft_rouge[metric] - base_rouge[metric]) / base_rouge[metric]) * 100\n",
        "    dpo_imp = ((sft_dpo_rouge[metric] - base_rouge[metric]) / base_rouge[metric]) * 100\n",
        "    print(f\"{metric:<15} {sft_imp:+.2f}%{'':<15} {dpo_imp:+.2f}%\")\n",
        "print(\"-\" * 80)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CALCULATING ROUGE SCORES\n",
            "================================================================================\n",
            "\n",
            "\ud83d\udcca ROUGE SCORES COMPARISON:\n",
            "--------------------------------------------------------------------------------\n",
            "Metric          Base            SFT             SFT+DPO        \n",
            "--------------------------------------------------------------------------------\n",
            "rouge1          0.1023          0.2405          0.2571         \n",
            "rouge2          0.0225          0.0846          0.0910         \n",
            "rougeL          0.0633          0.1490          0.1580         \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\ud83d\udcc8 IMPROVEMENT OVER BASE:\n",
            "--------------------------------------------------------------------------------\n",
            "Metric          SFT Improvement      SFT+DPO Improvement      \n",
            "--------------------------------------------------------------------------------\n",
            "rouge1          +135.11%                +151.35%\n",
            "rouge2          +276.12%                +304.42%\n",
            "rougeL          +135.25%                +149.46%\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CALCULATING BERTSCORE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "print(\"\\nCalculating BERTScore for base model...\")\n",
        "P_base, R_base, F1_base = bert_score(\n",
        "    base_predictions, references,\n",
        "    lang=\"en\", verbose=False,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "print(\"Calculating BERTScore for SFT model...\")\n",
        "P_sft, R_sft, F1_sft = bert_score(\n",
        "    sft_predictions, references,\n",
        "    lang=\"en\", verbose=False,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "print(\"Calculating BERTScore for SFT+DPO model...\")\n",
        "P_sft_dpo, R_sft_dpo, F1_sft_dpo = bert_score(\n",
        "    sft_dpo_predictions, references,\n",
        "    lang=\"en\", verbose=False,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "base_bertscore = {\n",
        "    'precision': P_base.mean().item(),\n",
        "    'recall': R_base.mean().item(),\n",
        "    'f1': F1_base.mean().item(),\n",
        "}\n",
        "\n",
        "sft_bertscore = {\n",
        "    'precision': P_sft.mean().item(),\n",
        "    'recall': R_sft.mean().item(),\n",
        "    'f1': F1_sft.mean().item(),\n",
        "}\n",
        "\n",
        "sft_dpo_bertscore = {\n",
        "    'precision': P_sft_dpo.mean().item(),\n",
        "    'recall': R_sft_dpo.mean().item(),\n",
        "    'f1': F1_sft_dpo.mean().item(),\n",
        "}\n",
        "\n",
        "print(\"\\n\ud83d\udcca BERTSCORE COMPARISON:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'Metric':<15} {'Base':<15} {'SFT':<15} {'SFT+DPO':<15}\")\n",
        "print(\"-\" * 80)\n",
        "for metric in ['precision', 'recall', 'f1']:\n",
        "    print(f\"{metric:<15} {base_bertscore[metric]:<15.4f} {sft_bertscore[metric]:<15.4f} {sft_dpo_bertscore[metric]:<15.4f}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print(\"\\n\ud83d\udcc8 IMPROVEMENT OVER BASE:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'Metric':<15} {'SFT Improvement':<20} {'SFT+DPO Improvement':<25}\")\n",
        "print(\"-\" * 80)\n",
        "for metric in ['precision', 'recall', 'f1']:\n",
        "    sft_imp = ((sft_bertscore[metric] - base_bertscore[metric]) / base_bertscore[metric]) * 100\n",
        "    dpo_imp = ((sft_dpo_bertscore[metric] - base_bertscore[metric]) / base_bertscore[metric]) * 100\n",
        "    print(f\"{metric:<15} {sft_imp:+.2f}%{'':<15} {dpo_imp:+.2f}%\")\n",
        "print(\"-\" * 80)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CALCULATING BERTSCORE\n",
            "================================================================================\n",
            "\n",
            "Calculating BERTScore for base model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c78d2f4fb1d947aa8b868cca7978f347",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "100bd011ea7d465ea018c4315ba136bf",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b7826e8301348ca89151e694d6b8fb6",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e803911579764e728d5001441a908d1f",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bebb4eb28bf8417d86d499bca295eb17",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0545c37a306f4aa28e80b26b8e2e32f7",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating BERTScore for SFT model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating BERTScore for SFT+DPO model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\ud83d\udcca BERTSCORE COMPARISON:\n",
            "--------------------------------------------------------------------------------\n",
            "Metric          Base            SFT             SFT+DPO        \n",
            "--------------------------------------------------------------------------------\n",
            "precision       0.6507          0.8008          0.8025         \n",
            "recall          0.6352          0.8045          0.8049         \n",
            "f1              0.6423          0.8024          0.8035         \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\ud83d\udcc8 IMPROVEMENT OVER BASE:\n",
            "--------------------------------------------------------------------------------\n",
            "Metric          SFT Improvement      SFT+DPO Improvement      \n",
            "--------------------------------------------------------------------------------\n",
            "precision       +23.06%                +23.32%\n",
            "recall          +26.64%                +26.71%\n",
            "f1              +24.93%                +25.10%\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Save Results & Create Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create detailed results DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'reference': references,\n",
        "    'base_prediction': base_predictions,\n",
        "    'sft_prediction': sft_predictions,\n",
        "    'sft_dpo_prediction': sft_dpo_predictions,\n",
        "})\n",
        "\n",
        "results_df.to_csv('model_comparison_results.csv', index=False)\n",
        "print(\"\u2705 Detailed results saved to 'model_comparison_results.csv'\")\n",
        "\n",
        "# Create summary DataFrame\n",
        "summary_data = {\n",
        "    'Metric': [\n",
        "        'ROUGE-1', 'ROUGE-2', 'ROUGE-L',\n",
        "        'BERTScore-Precision', 'BERTScore-Recall', 'BERTScore-F1'\n",
        "    ],\n",
        "    'Base Model': [\n",
        "        base_rouge['rouge1'], base_rouge['rouge2'], base_rouge['rougeL'],\n",
        "        base_bertscore['precision'], base_bertscore['recall'], base_bertscore['f1']\n",
        "    ],\n",
        "    'SFT Model': [\n",
        "        sft_rouge['rouge1'], sft_rouge['rouge2'], sft_rouge['rougeL'],\n",
        "        sft_bertscore['precision'], sft_bertscore['recall'], sft_bertscore['f1']\n",
        "    ],\n",
        "    'SFT+DPO Model': [\n",
        "        sft_dpo_rouge['rouge1'], sft_dpo_rouge['rouge2'], sft_dpo_rouge['rougeL'],\n",
        "        sft_dpo_bertscore['precision'], sft_dpo_bertscore['recall'], sft_dpo_bertscore['f1']\n",
        "    ],\n",
        "}\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "summary_df['SFT Improvement (%)'] = (\n",
        "    (summary_df['SFT Model'] - summary_df['Base Model']) / summary_df['Base Model'] * 100\n",
        ").round(2)\n",
        "summary_df['SFT+DPO Improvement (%)'] = (\n",
        "    (summary_df['SFT+DPO Model'] - summary_df['Base Model']) / summary_df['Base Model'] * 100\n",
        ").round(2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"FINAL EVALUATION SUMMARY\")\n",
        "print(\"=\"*100)\n",
        "print(summary_df.to_string(index=False))\n",
        "print(\"=\"*100)\n",
        "\n",
        "summary_df.to_csv('model_comparison_summary.csv', index=False)\n",
        "print(\"\\n\u2705 Summary saved to 'model_comparison_summary.csv'\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Detailed results saved to 'model_comparison_results.csv'\n",
            "\n",
            "====================================================================================================\n",
            "FINAL EVALUATION SUMMARY\n",
            "====================================================================================================\n",
            "             Metric  Base Model  SFT Model  SFT+DPO Model  SFT Improvement (%)  SFT+DPO Improvement (%)\n",
            "            ROUGE-1    0.102273   0.240453       0.257063               135.11                   151.35\n",
            "            ROUGE-2    0.022505   0.084644       0.091014               276.12                   304.42\n",
            "            ROUGE-L    0.063325   0.148968       0.157968               135.25                   149.46\n",
            "BERTScore-Precision    0.650731   0.800776       0.802454                23.06                    23.32\n",
            "   BERTScore-Recall    0.635228   0.804472       0.804909                26.64                    26.71\n",
            "       BERTScore-F1    0.642252   0.802378       0.803486                24.93                    25.10\n",
            "====================================================================================================\n",
            "\n",
            "\u2705 Summary saved to 'model_comparison_summary.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Sample Predictions Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"SAMPLE PREDICTIONS COMPARISON\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "num_samples_to_show = min(3, len(references))\n",
        "\n",
        "for i in range(num_samples_to_show):\n",
        "    print(f\"\\n{'='*100}\")\n",
        "    print(f\"SAMPLE {i+1}\")\n",
        "    print(\"=\"*100)\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcdd REFERENCE (Human-written):\")\n",
        "    print(f\"{references[i]}\")\n",
        "    \n",
        "    print(f\"\\n\ud83e\udd16 BASE MODEL:\")\n",
        "    print(f\"{base_predictions[i]}\")\n",
        "    \n",
        "    print(f\"\\n\ud83c\udfaf SFT MODEL:\")\n",
        "    print(f\"{sft_predictions[i]}\")\n",
        "    \n",
        "    print(f\"\\n\ud83c\udfc6 SFT+DPO MODEL:\")\n",
        "    print(f\"{sft_dpo_predictions[i]}\")\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Optional: Push Models to Hugging Face Hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Uncomment and fill in your details to push models to HF Hub\n",
        "\n",
        "# YOUR_HF_USERNAME = \"your_username\"\n",
        "# YOUR_HF_TOKEN = \"your_token\"\n",
        "\n",
        "# # Push SFT model\n",
        "# sft_model.push_to_hub(\n",
        "#     f\"{YOUR_HF_USERNAME}/llama3-spotlight-sft\",\n",
        "#     token=YOUR_HF_TOKEN,\n",
        "# )\n",
        "# sft_tokenizer.push_to_hub(\n",
        "#     f\"{YOUR_HF_USERNAME}/llama3-spotlight-sft\",\n",
        "#     token=YOUR_HF_TOKEN,\n",
        "# )\n",
        "\n",
        "# # Push SFT+DPO model\n",
        "# dpo_model.push_to_hub(\n",
        "#     f\"{YOUR_HF_USERNAME}/llama3-spotlight-sft-dpo\",\n",
        "#     token=YOUR_HF_TOKEN,\n",
        "# )\n",
        "# dpo_tokenizer.push_to_hub(\n",
        "#     f\"{YOUR_HF_USERNAME}/llama3-spotlight-sft-dpo\",\n",
        "#     token=YOUR_HF_TOKEN,\n",
        "# )\n",
        "\n",
        "# print(\"\u2705 Models pushed to Hugging Face Hub!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Training Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"COMPLETE PIPELINE SUMMARY\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "print(\"\\n\ud83d\udcca DATASET SPLIT:\")\n",
        "print(f\"  Total samples: {len(dataset)}\")\n",
        "print(f\"  SFT training: {len(sft_dataset)} ({len(sft_dataset)/len(dataset)*100:.1f}%)\")\n",
        "print(f\"  DPO training: {len(dpo_dataset)} ({len(dpo_dataset)/len(dataset)*100:.1f}%)\")\n",
        "print(f\"  Test: {len(test_dataset)} ({len(test_dataset)/len(dataset)*100:.1f}%)\")\n",
        "\n",
        "print(\"\\n\u23f1\ufe0f TRAINING TIME:\")\n",
        "print(f\"  SFT: {sft_stats.metrics['train_runtime']/60:.2f} minutes\")\n",
        "print(f\"  DPO: {dpo_stats.metrics['train_runtime']/60:.2f} minutes\")\n",
        "print(f\"  Total: {(sft_stats.metrics['train_runtime'] + dpo_stats.metrics['train_runtime'])/60:.2f} minutes\")\n",
        "\n",
        "print(\"\\n\ud83d\udcbe SAVED MODELS:\")\n",
        "print(\"  1. llama3_spotlight_sft (SFT only)\")\n",
        "print(\"  2. llama3_spotlight_sft_dpo (SFT + DPO)\")\n",
        "\n",
        "print(\"\\n\ud83d\udcc8 BEST PERFORMING MODEL:\")\n",
        "# Determine best model based on ROUGE-L F1\n",
        "scores = {\n",
        "    'Base': base_rouge['rougeL'],\n",
        "    'SFT': sft_rouge['rougeL'],\n",
        "    'SFT+DPO': sft_dpo_rouge['rougeL']\n",
        "}\n",
        "best_model = max(scores, key=scores.get)\n",
        "print(f\"  \ud83c\udfc6 {best_model} (ROUGE-L: {scores[best_model]:.4f})\")\n",
        "\n",
        "print(\"\\n\u2705 PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*100)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}